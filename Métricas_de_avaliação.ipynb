{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Avaliação\n",
    "\n",
    "## Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusão\n",
    "\n",
    "Matriz de Confusão é uma tabela composta por **n linhas** e **n colunas**, no qual, nas linhas tem-se seu valor real e nas colunas seu valor predito\n",
    "\n",
    "A imagem abaixo exemplifica uma matriz de confusão com apenas 2 classes.\n",
    "\n",
    "<img src=\"Metricas_de_avaliacao\Matriz_de_Confusão.jpg\">\n",
    "\n",
    "Supondo que temos que classificar um dataset com as classes *gato* e *não gato* e obtivemos a seguinte matriz de confusão:\n",
    "\n",
    "\n",
    "Matriz de Confusão (1)\n",
    "\n",
    "Real\\Predito | Gato | Não Gato \n",
    ":----|:----:|:----:\n",
    "Gato | 24 | 6 \n",
    "Não Gato | 4 | 16 \n",
    "\n",
    "\n",
    "Na primeira linha, tem-se os valores reais da classe 1 (Gato) pelo seu valor predito, onde podemos observar que:\n",
    "\n",
    "- 24 valores foram preditos de maneira correta (VP)\n",
    "- 6 de Maneira errada (FN)\n",
    "- obtendo um total de 30 amostras dessas classe\n",
    "\n",
    "Po sua vez, na segunda linha, temos os valores da classe 2 (não gato), ou seja:\n",
    "\n",
    "- 16 valores foram preditos de maneira correta (VN)\n",
    "- 4 de Maneira errada (FP)\n",
    "- obtendo um total de 20 amostras dessas classe\n",
    "\n",
    "Por fim, conseguimos saber que o total de amostras desse dataset foi de 50 amostras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia\n",
    "\n",
    "Acurácia é basicamente a quantidade de acertos pela quantidade total de amostras, ou seja, \n",
    "\n",
    "$$\n",
    "accuracy = \\frac{VP + VF}{qtd Amostras}\n",
    "$$\n",
    "\n",
    "No exemplo acima temos que a acurácia foi de \n",
    "\n",
    "$$\n",
    "\\frac{24 + 16}{50} = 0.8\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisão\n",
    "\n",
    "\n",
    "Precisão é, basicamente, a quantidade de verdadeiros positivos dividido por ele mesmo, somado pela quantidade de falsos positivos. Ou seja, temos:\n",
    "\n",
    "$$\n",
    "precision = \\frac{VP}{VP + FP}\n",
    "$$\n",
    "\n",
    "Nesse caso temos:\n",
    "\n",
    "$$\n",
    "precision = \\frac{24}{24 + 4} = 0.857\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revocação\n",
    "\n",
    "**Quanto maior Recall, menor a chance de Falsos Negativos**\n",
    "\n",
    "Quantidade de verdadeiros positivos, dividido pela quantidade total de exemplos que pertencem a esta classe, mesmo que sejam classificados em outra. Ou seja, verdadeiros positivos divididos por total de positivos.\n",
    "\n",
    "$$\n",
    "recall = \\frac{VP}{VP + FN}\n",
    "$$\n",
    "\n",
    "Nesse caso temos:\n",
    "\n",
    "$$\n",
    "recall = \\frac{24}{24 + 6} = 0.8\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pontuação F1\n",
    "\n",
    "F1 Score é a média harmônica entre Precision e Recall e é **muito boa para datasets desbalanceados**, uma vez que é uma relação entre precisão e recall\n",
    "\n",
    "$$\n",
    "F1 score = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}\n",
    "$$\n",
    "\n",
    "Nesse caso temos:\n",
    "\n",
    "$$\n",
    "F1 score = \\frac{2*0.857*0.8}{0.857 + 0.8} = 0,827\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC - AUC\n",
    "*Area Under the ROC Curve*\n",
    "\n",
    "É a área da curva formada pelo gráfico entre VP e FP\n",
    "\n",
    "<img src=\"AUC_ROC.png\">\n",
    "\n",
    "Quanto maior a AUC, melhor será o modelo em prever 0s como 0s e 1s como 1s. \n",
    "\n",
    "- Um modelo excelente tem AUC próximo de 1, o que significa que tem uma boa medida de separabilidade. \n",
    "- Um modelo pobre tem AUC próximo de 0, o que significa que ele tem a pior medida de separabilidade. Na verdade, significa que está retribuindo o resultado. Ele está prevendo 0s como 1s e 1s como 0s. \n",
    "- E quando AUC é 0,5, significa que o modelo não tem capacidade de separação de classes de qualquer espécie.\n",
    "\n",
    "Uma das vantagens em relação ao F1 Score, é que ela mede o desempenho do modelo em vários pontos de corte, **não necessariamente atribuindo exemplos com probabilidade maior que 50% para a classe positiva, e menor, para a classe negativa**.\n",
    "\n",
    "Abaixo tem-se os exemplos de medições de AUC.\n",
    "\n",
    "AUC = 1\n",
    "\n",
    "<img src=\"AUC_1.png\"> \n",
    "<img src=\"ROC_1.png\">\n",
    "                    \n",
    "AUC = 0.7\n",
    "\n",
    "<img src=\"AUC_07.png\">\n",
    "<img src=\"ROC_07.png\">\n",
    "\n",
    "AUC = 0.5\n",
    "\n",
    "<img src=\"AUC_05.png\">\n",
    "<img src=\"ROC_05.png\">\n",
    "\n",
    "AUC = 0\n",
    "<img src=\"AUC_0.png\">\n",
    "<img src=\"ROC_0.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "name": "python370jvsc74a57bd0b3c1950c2a4722206cbb7af0d570e2d4d2ccba701ca75028dee5665d6f9ab7e2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "b3c1950c2a4722206cbb7af0d570e2d4d2ccba701ca75028dee5665d6f9ab7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
